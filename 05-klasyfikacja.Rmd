# Klasyfikacja

[A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

## Drzewa klasyfikacyjne

Zalety:

- łatwa interpretacja
- nie trzeba normalizować cech
- rozwiązuje problemy liniowe i nieliniowe

Wady:

- mała efektywność przy małych zbiorach danych
- łatwo można przeuczyć


## KNN

Algorytm:

1. Określ liczbę sąsiadów - $K$
2. Wyznacz $K$ sąsiadów dla nowego punktu na podstawie wybranej odległości
3. Oblicz liczbę sąsiadów, w każdej z grup
4. Przypisz nową obserwację do grupy, w której ma więcej najbliższych sąsiadów

Zalety:

- łatwa interpretacja
- szybki i efektywny

Wady:

- trzeba określić liczbę sąsiadów


### Zadanie

Zbuduj model klasyfikacyjny dla zbioru [danych](data/Social_Network_Ads.csv) dotyczących cech internautów oraz informacji czy zamówili reklamowany produkt czy nie.

Przeprowadź imputację braków danych dla zbioru [pracowników](data/pracownicy.RData).

```{r eval=FALSE, include=FALSE}
library(tidyverse)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)

homes <- read.csv("data/part_1_data.csv")

podzial <- sample.split(Y = homes$in_sf, SplitRatio = 0.75)

train <- homes[podzial,]
test <- homes[!podzial,]

drzewo <- rpart(in_sf ~ ., data = train, method = "class")

rpart.plot(drzewo)

printcp(drzewo)

in_sf_test <- predict(drzewo, newdata = test, 
                      type = "class")
in_sf_test

cm <- table(test$in_sf, in_sf_test)
cm

confusionMatrix(cm)

# zbiór uczący

in_sf_train <- predict(drzewo, newdata = train, 
                       type = "class")

table(train$in_sf, in_sf_train)

# obcięcie

optimum <- drzewo$cptable[which.min(drzewo$cptable[,"xerror"]),"CP"]
optimum

drzewo_opt <- prune(drzewo, cp = optimum)

rpart.plot(drzewo_opt)

in_sf_train <- predict(drzewo_opt, newdata = train, 
                       type = "class")

table(train$in_sf, in_sf_train)

in_sf_test <- predict(drzewo_opt, newdata = test, 
                      type = "class")

table(test$in_sf, in_sf_test)

```

