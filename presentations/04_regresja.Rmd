---
title: "Metody przetwarzania<br>i analizy danych"
subtitle: "Regresja liniowa"
author: "&copy; Łukasz Wawrowski"
date: ""
output:
  xaringan::moon_reader:
    css: ["default.css", "default-fonts.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r include=FALSE}
library(tidyverse)
library(countdown)

salary <- readxl::read_xlsx("../data/salary.xlsx")

knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.width = 12, fig.height = 6)
```

# Regresja

Funkcyjne odwzorowanie zależności pomiędzy badanymi zmiennymi. 

Cele analizy regresji:

- poznawcze - badanie związków przyczynowo-skutkowych

- predycyjne - oszacowanie nieznanej wartości cechy

Model regresji jest tylko przybliżeniem rzeczywistości!

---

# Regresja prosta

Analiza dwóch cech:

- zmienna objaśniana (zależna) oznaczana jako $y$

- zmienna objaśniająca (niezależna) oznaczana jako $x$

Przykłady: 

- zależność wielkości sprzedaży od wydatków na reklamę

- zależność wynagrodzenia od lat doświadczenia

---

# Przykład

Zbiór [salary](http://www.wawrowski.edu.pl/data/salary.xlsx) zawiera informacje o rocznym wynagrodzeniu (w $) oraz liczbie lat doświadczenia. 

---

# Wykres rozrzutu

```{r echo=FALSE}
plot(salary)
```

---

class: inverse

# Zadanie

Stwórz wykres rozrzutu dla zbioru `salary` z wykorzystaniem pakietu _ggplot2_.

`r countdown(minutes = 5, seconds = 0, top = 0)`

---

# Wykres rozrzutu

```{r echo=FALSE}

ggplot(salary, aes(x = YearsExperience, y = Salary)) + 
  geom_point() +
  geom_smooth(method = "lm")

```

---

# Regresja prosta

Weźmy pod uwagę prosty przykład dochodów i wydatków:

```{r echo=FALSE}
d <- data.frame(wydatki=c(2300,1800,2400,2300,2800,2000,2100),
                dochody=c(2600,2400,2900,2800,3000,2500,2700))

d %>% knitr::kable()
```

---

# Regresja prosta

Wykres rozrzutu

```{r echo=FALSE}
ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

Spróbujmy teraz dopasować kilka prostych - mogą one przebiegać na wiele różnych sposobów.

```{r echo=FALSE}
ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

W następnym kroku obliczamy różnice pomiędzy istniejącymi punktami, a odpowiadającym im wartościom na prostej: 

```{r echo=FALSE}
d <- d %>% 
  mutate(niebieska_y=dochody-2243,
         zielona_y=1.9*dochody-2780,
         czerwona_y=1.357*dochody-1421.429) %>% 
  mutate(niebieska=(wydatki-niebieska_y)^2,
         zielona=(wydatki-zielona_y)^2,
         czerwona=(wydatki-czerwona_y)^2)

line1 <- data.frame(x = c(d$dochody[1], d$dochody[1]), y=c(d$wydatki[1], d$czerwona_y[1]))
line2 <- data.frame(x = c(d$dochody[2], d$dochody[2]), y=c(d$wydatki[2], d$czerwona_y[2]))
line3 <- data.frame(x = c(d$dochody[3], d$dochody[3]), y=c(d$wydatki[3], d$czerwona_y[3]))
line4 <- data.frame(x = c(d$dochody[4], d$dochody[4]), y=c(d$wydatki[4], d$czerwona_y[4]))
line5 <- data.frame(x = c(d$dochody[5], d$dochody[5]), y=c(d$wydatki[5], d$czerwona_y[5]))
line6 <- data.frame(x = c(d$dochody[6], d$dochody[6]), y=c(d$wydatki[6], d$czerwona_y[6]))
line7 <- data.frame(x = c(d$dochody[7], d$dochody[7]), y=c(d$wydatki[7], d$czerwona_y[7]))

ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  geom_line(data = line1, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line2, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line3, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line4, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line5, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line6, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line7, aes(x=x, y=y), color = "grey60", size = 1.2) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

Oznaczając $y_i$ jako rzeczywista wartość wydatków i $\hat{y_i}$ jako wartość leżącą na prostej zależy nam na minimalizowaniu wyrażenia $\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2} \rightarrow min$. Różnica $y_{i}-\hat{y}_{i}$ jest nazywana resztą (ang. residual). Wyznaczając te wartości dla analizowanych przez nas prostych otrzymamy następujące wyniki:

```{r echo=FALSE}
d %>% 
  select(6:8) %>% 
  pivot_longer(1:3) %>% 
  group_by(name) %>% 
  summarise(suma_kwadratow_reszt=round(sum(value))) %>% 
  arrange(suma_kwadratow_reszt) %>% 
  knitr::kable()
```

---

# Regresja prosta

Ogólna postać regresji prostej jest następująca:

$$\hat{y}_{i}=b_{1}x_{i}+b_{0}$$

gdzie $\hat{y}$ oznacza wartość teoretyczną, leżącą na wyznaczonej prostej. 

Wobec tego wartości empiryczne (y) będą opisane formułą:

$$y_{i}=b_{1}x_{i}+b_{0}+u_{i}$$

w której $u_i$ oznacza składnik resztowy wyliczany jako $u_{i}=y_{i}-\hat{y}_{i}$. 

---

# Regresja prosta w R

```{r eval=FALSE}
lm(formula = zmienna_zalezna ~ zmienna_niezalezna, data = zbior_danych)
```

- `formula` - zdefiniowanie zależności funkcyjnej z wykorzystaniem tyldy

- `data` - zbiór danych

Domyślnie funkcja `lm` zwraca tylko parametry $b$. 

Aby uzyskać szczegółowe informacje na temat modelu należy dodatkowo zastosować funkcję `summary()`:

```{r eval=FALSE}
model <- lm(formula = zmienna_zalezna ~ zmienna_niezalezna, data = zbior_danych)
summary(model)
```

---

# Współczynniki $b$

**Współczynnik kierunkowy** $b_1$ informuje o ile przeciętne zmieni się wartość zmiennej objaśnianej $y$, gdy wartość zmiennej objaśniającej $x$ wzrośnie o jednostkę.

**Wyraz wolny** $b_0$ to wartość zmiennej objaśnianej $y$, w sytuacji w której wartość zmiennej objaśniającej $x$ będzie równa 0. Często interpretacja tego parametru nie ma sensu.

---

# Dopasowanie modelu

**Odchylenie standardowe składnika resztowego** jest pierwiastkiem z sumy kwadratów reszt podzielonej przez liczbę obserwacji pomniejszoną o 2:

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{n-2}}$$

Miara ta określa, o ile, przeciętnie biorąc $+/-$, wartości empiryczne zmiennej objaśnianej odchylają się od wartości teoretycznych tej zmiennej, obliczonej na podstawie funkcji regresji.

---

# Dopasowanie modelu 

**Współczynnik determinacji** określa, jaki procent wariancji zmiennej objaśnianej został wyjaśniony przez funkcję regresji. $R^2$ przyjmuje wartości z przedziału $<0;1>$ ( $<0\%;100\%>$ ), przy czym model regresji tym lepiej opisuje zachowanie się badanej zmiennej objaśnianej, im $R^2$ jest bliższy jedności (bliższy 100%)

$$R^2=1-\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

---

# Test Walda

- sprawdzenie istotności parametrów $b$

- sprawdzenie istotności całego wektora parametrów $b$

---

# Predykcja

W celu wykorzystania modelu regresji do prognozowania należy stworzyć lub wczytać zbiór danych z danymi, dla których chcemy uzyskać wartości.

```{r eval=FALSE}
nowe_dane <- data.frame(x=c(10,20,30))

predict(object = model, newdata = nowe_dane)
```

---

class: inverse

# Zadanie 

Stwórz model regresji prostej objaśniający zależność sprzedaży od liczby klientów na podstawie [sklepu Rossmann](http://www.wawrowski.edu.pl/data/sklep77.xlsx). Jaka jest prognozowana sprzedaż dla 300, 700 i 1050 klientów?

`r countdown(minutes = 10, seconds = 0, top = 0)`

---

# Regresja wieloraka

Ogólna postać regresji wielorakiej jest następująca:

$$\hat{y}_{i}=b_{1}x_{1i}+b_{2}x_{2i}+...+b_{k}x_{ki}+b_{0}$$

W tym przypadku nie wyznaczamy prostej tylko $k$-wymiarową przestrzeń.

---

# Regresja trzech zmiennych

![](img/multiple_reg.png)

---

# Przykład

Na podstawie zbioru [pracownicy](http://www.wawrowski.edu.pl/data/pracownicy.xlsx) zbuduj model objaśniający wysokość bieżącego wynagrodzenia.

- id - kod pracownika
- plec - płeć pracownika (0 - mężczyzna, 1 - kobieta)
- data_urodz - data urodzenia
- edukacja - wykształcenie (w latach nauki)
- kat_pracownika - grupa pracownicza (1 - specjalista, 2 - menedżer, 3 - konsultant)
- bwynagrodzenie - bieżące wynagrodzenie
- pwynagrodzenie - początkowe wynagrodzenie
- staz - staż pracy (w miesiącach)
- doswiadczenie - poprzednie zatrudnienie (w miesiącach)
- zwiazki - przynależność do związków zawodowych (0 - nie, 1 - tak)
- wiek - wiek (w latach)

---

class: inverse

# Zadanie

Na bazie zbioru pracownicy stwórz nowy zbiór danych, który nie będzie zawierał niepotrzebnych zmiennych oraz braków danych.

`r countdown(minutes = 5, seconds = 0, top = 0)`

---

# Dychotomizacja zmiennej

Zamiana zmiennej ilościowej zawierającej $k$ wariantów na $k-1$ zmiennych zerojedynkowych.

.pull-left[
Oryginalny zbiór

| id | stanowisko |
|----|------------|
| 1  | specjalista|
| 2  | menedżer   |
| 3  | specjalista|
| 4  | konsultant |
| 5  | konsultant |

]

.pull-right[
Zmienna zerojedynkowa

| id | menedżer | konsultant |
|----|----------|------------|
| 1  | 0 | 0 |
| 2  | 1 | 0 |
| 3  | 0 | 0 |
| 4  | 0 | 1 |
| 5  | 0 | 1 |

]

---

# Dychotomizacja zmiennej

Współczynnik $b$ dla zmiennej dychotomicznej informuje o ile przeciętne zmieni się wartość zmiennej objaśnianej $y$ w odniesieniu do kategorii bazowej dychotomicznej zmiennej $x$.

Przykładowo, przyjmując za kategorię bazową stanowisko _specjalista_, współczynnik $b$ dla kategorii _menedżer_ poinformuje o ile średnio wartość bieżącego wynagrodzenia jest wyższa lub niższa od _specjalisty_.

---

# Badanie współliniowości

Pakiet [corrplot](https://cran.r-project.org/web/packages/corrplot/) służący do wizualizacji współczynnika korelacji.

Współczynnik korelacji informuje o sile zależności pomiędzy dwoma cechami ilościowymi. Jest wielkością unormowaną, przyjmuje wartości z przedziału $r\in<-1;1>$.

Jeśli:

- $r_{xy}=1$ --- korelacja dodatnia doskonała,
- $0<r_{xy}<1$ --- korelacja dodatnia niedoskonała (słaba/umiarkowana/silna)
- $r_{xy}=0$ --- brak zależności,
- $-1<r_{xy}<0$ --- korelacja ujemna niedoskonała (słaba/umiarkowana/silna)
- $r_{xy}=-1$ --- korelacja ujemna doskonała.

---

# Dobór i weryfikacja modelu

Pakiet [olsrr](https://cran.r-project.org/web/packages/olsrr/) zawiera narzędzia do analizy modeli liniowych.

- wybór cech do modelu

- badanie współliniowości

- badanie normalności

- analiza wartości odstających

---

# Dobór modelu

Wyróżnia się trzy podejścia do tego zagadnienia:

- ekspercki dobór cech

- budowa wszystkich możliwych modeli i wybór najlepszego według określonego kryterium

- regresja krokowa

---

# Badanie współliniowości

**Współczynnik tolerancji** wskazuje na procent niewyjaśnionej zmienności danej zmiennej przez pozostałe zmienne objaśniające.

**Współczynnik VIF** jest obliczany na podstawie wartości współczynnika tolerancji i wskazuje o ile wariancja szacowanego współczynnika regresji jest podwyższona z powodu współliniowości danej zmiennej objaśniającej z pozostałymi zmiennymi objaśniającymi. Wartość współczynnika VIF powyżej 4 należy uznać za wskazującą na współliniowość.

---

class: inverse

# Zadanie

Na podstawie zbioru dotyczącego [50 startupów](http://www.wawrowski.edu.pl/data/startups.xlsx) określ jakie czynniki wpływają na przychód startupów. 

Przyda się pakiet _janitor_ i funkcja `clean_names()` do uporządkowania nazw kolumn.

`r countdown(minutes = 10, seconds = 0, top = 0)`

---

class: inverse, center, middle

# Pytania?