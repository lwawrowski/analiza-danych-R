---
title: "Metody przetwarzania<br>i analizy danych"
subtitle: "Regresja liniowa"
author: "&copy; Łukasz Wawrowski"
date: ""
output:
  xaringan::moon_reader:
    css: ["default.css", "default-fonts.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r include=FALSE}
library(tidyverse)
library(countdown)

salary <- readxl::read_xlsx("../data/salary.xlsx")

knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.width = 12, fig.height = 6)
```

# Regresja

Funkcyjne odwzorowanie zależności pomiędzy badanymi zmiennymi. 

Cele analizy regresji:

- poznawcze - badanie związków przyczynowo-skutkowych

- predycyjne - oszacowanie nieznanej wartości cechy

Model regresji jest tylko przybliżeniem rzeczywistości!

---

# Regresja prosta

Analiza dwóch cech:

- zmienna objaśniana (zależna) oznaczana jako $y$

- zmienna objaśniająca (niezależna) oznaczana jako $x$

Przykłady: 

- zależność wielkości sprzedaży od wydatków na reklamę

- zależność wynagrodzenia od lat doświadczenia

---

# Przykład

Zbiór [salary](http://www.wawrowski.edu.pl/data/salary.xlsx) zawiera informacje o rocznym wynagrodzeniu (w $) oraz liczbie lat doświadczenia. 

---

# Wykres rozrzutu

```{r echo=FALSE}
plot(salary)
```

---

class: inverse

# Zadanie

Stwórz wykres rozrzutu dla zbioru `salary` z wykorzystaniem pakietu _ggplot2_.

`r countdown(minutes = 5, seconds = 0, top = 0)`

---

# Wykres rozrzutu

```{r echo=FALSE}

ggplot(salary, aes(x = YearsExperience, y = Salary)) + 
  geom_point() +
  geom_smooth(method = "lm")

```

---

# Regresja prosta

Weźmy pod uwagę prosty przykład dochodów i wydatków:

```{r echo=FALSE}
d <- data.frame(wydatki=c(2300,1800,2400,2300,2800,2000,2100),
                dochody=c(2600,2400,2900,2800,3000,2500,2700))

d %>% knitr::kable()
```

---

# Regresja prosta

Wykres rozrzutu

```{r echo=FALSE}
ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

Spróbujmy teraz dopasować kilka prostych - mogą one przebiegać na wiele różnych sposobów.

```{r echo=FALSE}
ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

W następnym kroku obliczamy różnice pomiędzy istniejącymi punktami, a odpowiadającym im wartościom na prostej: 

```{r echo=FALSE}
d <- d %>% 
  mutate(niebieska_y=dochody-2243,
         zielona_y=1.9*dochody-2780,
         czerwona_y=1.357*dochody-1421.429) %>% 
  mutate(niebieska=(wydatki-niebieska_y)^2,
         zielona=(wydatki-zielona_y)^2,
         czerwona=(wydatki-czerwona_y)^2)

line1 <- data.frame(x = c(d$dochody[1], d$dochody[1]), y=c(d$wydatki[1], d$czerwona_y[1]))
line2 <- data.frame(x = c(d$dochody[2], d$dochody[2]), y=c(d$wydatki[2], d$czerwona_y[2]))
line3 <- data.frame(x = c(d$dochody[3], d$dochody[3]), y=c(d$wydatki[3], d$czerwona_y[3]))
line4 <- data.frame(x = c(d$dochody[4], d$dochody[4]), y=c(d$wydatki[4], d$czerwona_y[4]))
line5 <- data.frame(x = c(d$dochody[5], d$dochody[5]), y=c(d$wydatki[5], d$czerwona_y[5]))
line6 <- data.frame(x = c(d$dochody[6], d$dochody[6]), y=c(d$wydatki[6], d$czerwona_y[6]))
line7 <- data.frame(x = c(d$dochody[7], d$dochody[7]), y=c(d$wydatki[7], d$czerwona_y[7]))

ggplot(d, aes(x = dochody, y = wydatki)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  geom_line(data = line1, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line2, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line3, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line4, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line5, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line6, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line7, aes(x=x, y=y), color = "grey60", size = 1.2) +
  xlab("dochody (X)") + 
  ylab("wydatki (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

---

# Regresja prosta

Oznaczając $y_i$ jako rzeczywista wartość wydatków i $\hat{y_i}$ jako wartość leżącą na prostej zależy nam na minimalizowaniu wyrażenia $\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2} \rightarrow min$. Różnica $y_{i}-\hat{y}_{i}$ jest nazywana resztą (ang. residual). Wyznaczając te wartości dla analizowanych przez nas prostych otrzymamy następujące wyniki:

```{r echo=FALSE}
d %>% 
  select(6:8) %>% 
  pivot_longer(1:3) %>% 
  group_by(name) %>% 
  summarise(suma_kwadratow_reszt=round(sum(value))) %>% 
  arrange(suma_kwadratow_reszt) %>% 
  knitr::kable()
```

---

# Regresja prosta

Ogólna postać regresji prostej jest następująca:

$$\hat{y}_{i}=b_{1}x_{i}+b_{0}$$

gdzie $\hat{y}$ oznacza wartość teoretyczną, leżącą na wyznaczonej prostej. 

Wobec tego wartości empiryczne (y) będą opisane formułą:

$$y_{i}=b_{1}x_{i}+b_{0}+u_{i}$$

w której $u_i$ oznacza składnik resztowy wyliczany jako $u_{i}=y_{i}-\hat{y}_{i}$. 

---

# Regresja prosta w R

```{r eval=FALSE}
lm(formula = zmienna_zalezna ~ zmienna_niezalezna, data = zbior_danych)
```

- `formula` - zdefiniowanie zależności funkcyjnej z wykorzystaniem tyldy

- `data` - zbiór danych

Domyślnie funkcja `lm` zwraca tylko parametry $b$. 

Aby uzyskać szczegółowe informacje na temat modelu należy dodatkowo zastosować funkcję `summary()`:

```{r eval=FALSE}
model <- lm(formula = zmienna_zalezna ~ zmienna_niezalezna, data = zbior_danych)
summary(model)
```

---

# Współczynniki $b$

**Współczynnik kierunkowy** $b_1$ informuje o ile przeciętne zmieni się wartość zmiennej objaśnianej $y$, gdy wartość zmiennej objaśniającej $x$ wzrośnie o jednostkę.

**Wyraz wolny** $b_0$ to wartość zmiennej objaśnianej $y$, w sytuacji w której wartość zmiennej objaśniającej $x$ będzie równa 0. Często interpretacja tego parametru nie ma sensu.

---

# Dopasowanie modelu

**Odchylenie standardowe składnika resztowego** jest pierwiastkiem z sumy kwadratów reszt podzielonej przez liczbę obserwacji pomniejszoną o 2:

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{n-2}}$$

Miara ta określa, o ile, przeciętnie biorąc $+/-$, wartości empiryczne zmiennej objaśnianej odchylają się od wartości teoretycznych tej zmiennej, obliczonej na podstawie funkcji regresji.

---

# Dopasowanie modelu 

**Współczynnik determinacji** określa, jaki procent wariancji zmiennej objaśnianej został wyjaśniony przez funkcję regresji. $R^2$ przyjmuje wartości z przedziału $<0;1>$ ( $<0\%;100\%>$ ), przy czym model regresji tym lepiej opisuje zachowanie się badanej zmiennej objaśnianej, im $R^2$ jest bliższy jedności (bliższy 100%)

$$R^2=1-\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

---

# Test Walda

- sprawdzenie istotności parametrów $b$

- sprawdzenie istotności całego wektora parametrów $b$

---

# Inne miary jakości

Do analizy jakości modelu można także wykorzystać inne miary obliczane na podstawie wartości rzeczywistych oraz predykcji. Te najpopularniejsze zaimplementowane są w pakiecie [mlr3measures](https://cran.r-project.org/web/packages/mlr3measures/).

- MAE - Mean Absolute Error

- MAPE - Mean Absolute Percentage Error

- MSE - Mean Squared Error

- RMSE - Root Mean Squared Error

W pliku pomocy, dla każdej miary jest określony jej wzór, informacja o możliwych wartościach oraz kierunku pożądnych wartości (minimalizacja czy maksymalizacja).

---

# Predykcja

W celu wykorzystania modelu regresji do prognozowania należy stworzyć lub wczytać zbiór danych z danymi, dla których chcemy uzyskać wartości.

```{r eval=FALSE}
nowe_dane <- data.frame(x=c(10,20,30))

predict(object = model, newdata = nowe_dane)
```

---

class: inverse

# Zadanie 

Stwórz model regresji prostej objaśniający zależność sprzedaży od liczby klientów na podstawie [sklepu Rossmann](http://www.wawrowski.edu.pl/data/sklep77.xlsx). Jaka jest prognozowana sprzedaż dla 300, 700 i 1050 klientów?

`r countdown(minutes = 10, seconds = 0, top = 0)`

---

# Regresja wieloraka

Ogólna postać regresji wielorakiej jest następująca:

$$\hat{y}_{i}=b_{1}x_{1i}+b_{2}x_{2i}+...+b_{k}x_{ki}+b_{0}$$

W tym przypadku nie wyznaczamy prostej tylko $k$-wymiarową przestrzeń.

---

# Regresja trzech zmiennych

![](img/multiple_reg.png)

---

# Przykład

Na podstawie zbioru [pracownicy](http://www.wawrowski.edu.pl/data/pracownicy.xlsx) zbuduj model objaśniający wysokość bieżącego wynagrodzenia.

- id - kod pracownika
- plec - płeć pracownika (0 - mężczyzna, 1 - kobieta)
- data_urodz - data urodzenia
- edukacja - wykształcenie (w latach nauki)
- kat_pracownika - grupa pracownicza (1 - specjalista, 2 - menedżer, 3 - konsultant)
- bwynagrodzenie - bieżące wynagrodzenie
- pwynagrodzenie - początkowe wynagrodzenie
- staz - staż pracy (w miesiącach)
- doswiadczenie - poprzednie zatrudnienie (w miesiącach)
- zwiazki - przynależność do związków zawodowych (0 - nie, 1 - tak)
- wiek - wiek (w latach)

---

### Mechanizmy powstawania braków danych

**Całkowicie losowy - Missing Completely at Random (MCAR)**

- Proces powstawania braków jest całkowicie losowy.

**Losowy - Missing at Random (MAR)**

- Braki danych zależą od zmiennych obserwowanych np. mężczyźni odmawiają odpowiedzi w pytaniach o depresję, a kobiety o wagę. 
- Można sprawdzić np. testem $\chi^2$.
- Lepszym określeniem byłoby Missing Conditionally at Random

**Nielosowy - Missing not at Random (MNAR)**

- Braki danych zależą od zmiennych obserwowalnych i nieobserwowalnych np. inteligencji, statusu społecznego.

[Rubin, D. (1976). Inference and Missing Data. Biometrika, 63(3), 581-592](https://www.jstor.org/stable/2335739)

---

# Możliwe rozwiązania

- usunięcie obserwacji z brakami danych

- imputacja statystyczna:

   - metody deterministyczne - zawsze te same wartości imputacyjne:
      
      - zastępowanie wartością przeciętną, imputacja regresyjna,
      - nie wprowadzają dodatkowego źródła błędu losowego,
      - zniekształcają rozkłady zmiennych
   
   - metody stochastyczne - można uzyskać różne wartości imputacyjne:
   
      - metoda hot-deck, stochastyczna imputacja regresyjna,
      - generują dodatkowy błąd,
      - lepiej zachowują rozkłady zmiennych

[Tomasz Piasecki - Imputacja dochodów w badaniach statystyki publicznej dotyczących gospodarstw domowych](https://stat.gov.pl/cps/rde/xbcr/lodz/ASSETS_Konferencja_MRS_2013_plakat_Piasecki_T_Imputacja_dochodow_w_badaniach_statystyki_publicznej.pdf)

---

# Zastępowanie braków danych

Zmienna ilościowa

```{r eval=FALSE}
dane <- dane_braki %>%
  mutate(zmienna=if_else(condition = is.na(zmienna), 
                         true = mean(zmienna, na.rm = T), 
                         false = zmienna))
```


Zmienna jakościowa

```{r eval=FALSE}
dominanta <- names(sort(table(dane_braki$zmienna), decreasing = T))[1]

dane <- dane_braki %>%
  mutate(zmienna=if_else(condition = is.na(zmienna), 
                         true = dominanta, 
                         false = zmienna))
```

---

# Pakiet VIM

[Visualization and Imputation of Missing Values](https://cran.r-project.org/web/packages/VIM/index.html)

[Alexander Kowarik, Matthias Templ (2016). Imputation with the R Package VIM. Journal of Statistical Software, 74(7), 1-16.](https://www.jstatsoft.org/article/view/v074i07/v74i07.pdf)

- implementuje najpopularniejsze metody do zastępowania braków danych,

- funkcje z pakietu zwracają zbiór danych z uzupełnionymi wartościami i flagą dla imputowanych obserwacji

---

# Metoda hot-deck

Dla każdego braku danych poszukiwany jest _dawca_ - obserwacja podobna pod kątem cech skorelowanych ze zmienną (pomocniczych), która zawiera brak danych. 

Warianty:

- losowa - stworzenie grup imputacyjnych na podstawie cech pomocniczych. Dawca wybierany jest losowo z odpowiedniej grupy.

- sekwencyjna - uporządkowanie danych na podstawie cech pomocniczych. Dawca wybierany jest jako pierwsza obserwacja o takich samych cechach pomocniczych.

---

# Metoda hot-deck

Funkcja `hotdeck`

```{r eval=FALSE}
hotdeck(data = zbior, variable = "cecha", 
        ord_var = c("zmienna1", "zmienna2"), domain_var = "domena")
```

- `data` - zbiór danych wejściowych

- `variable` - cecha/cechy do imputacji

- `ord_var` - zmienne pomocnicze

- `domain_var` - zmienne domenowe - w ramach, których ma być przeprowadzona imputacja

---

# Metoda najbliższych sąsiadów

Poszukiwanie obserwacji dawcy na podstawie wybranych zmiennych oraz kryterium minimalnej odległości dla wybranej liczby $k$ sąsiadów.

Obliczanie odległości opiera się na metryce Gowera (1971), która umożliwia obliczanie odległości dla cech jakościowych i ilościowych.

Imputowana wartość dla cech ilościowych to mediana z wartości $k$ sąsiadów, a dla cech jakościowych najczęściej występujący wariant.

---

# Metoda najbliższych sąsiadów

Funkcja `kNN`

```{r eval=FALSE}
kNN(data = zbior, variable = c("cecha1", "cecha2"), 
    dist_var = c("zmienna1", "zmienna2", "zmienna3"), k = 5)
```

- `data` - zbiór danych wejściowych

- `variable` - cecha/cechy do imputacji

- `dist_var` - zmienne wykorzystywane do obliczenia odległości

- `k` - liczba sąsiadów

---

# Imputacja regresyjna

Zastąpienie braków danych wartościami teoretycznymi z modelu liniowego.

Funkcja `regressionImp`

```{r eval=FALSE}
regressionImp(formula = model, data = zbior)
```

- `formula` - model dla zmiennej imputowanej

- `data` - zbiór danych wejściowych

Podobnie działa funkcja `rangerImpute()` wykorzystująca lasy losowe, natomiast przy domyślnych parametrach nie będzie metodą deterministyczną.


---

class: inverse

# Zadanie

Na bazie zbioru pracownicy stwórz nowy zbiór danych, który nie będzie zawierał niepotrzebnych zmiennych oraz braków danych.

`r countdown(minutes = 5, seconds = 0, top = 0)`

---

# Dychotomizacja zmiennej

Zamiana zmiennej ilościowej zawierającej $k$ wariantów na $k-1$ zmiennych zerojedynkowych.

.pull-left[
Oryginalny zbiór

| id | stanowisko |
|----|------------|
| 1  | specjalista|
| 2  | menedżer   |
| 3  | specjalista|
| 4  | konsultant |
| 5  | konsultant |

]

.pull-right[
Zmienna zerojedynkowa

| id | menedżer | konsultant |
|----|----------|------------|
| 1  | 0 | 0 |
| 2  | 1 | 0 |
| 3  | 0 | 0 |
| 4  | 0 | 1 |
| 5  | 0 | 1 |

]

---

# Dychotomizacja zmiennej

Współczynnik $b$ dla zmiennej dychotomicznej informuje o ile przeciętne zmieni się wartość zmiennej objaśnianej $y$ w odniesieniu do kategorii bazowej dychotomicznej zmiennej $x$.

Przykładowo, przyjmując za kategorię bazową stanowisko _specjalista_, współczynnik $b$ dla kategorii _menedżer_ poinformuje o ile średnio wartość bieżącego wynagrodzenia jest wyższa lub niższa od _specjalisty_.

---

# Badanie współliniowości

Pakiet [corrplot](https://cran.r-project.org/web/packages/corrplot/) służący do wizualizacji współczynnika korelacji.

Współczynnik korelacji informuje o sile zależności pomiędzy dwoma cechami ilościowymi. Jest wielkością unormowaną, przyjmuje wartości z przedziału $r\in<-1;1>$.

Jeśli:

- $r_{xy}=1$ - korelacja dodatnia doskonała,
- $0<r_{xy}<1$ - korelacja dodatnia niedoskonała (słaba/umiarkowana/silna)
- $r_{xy}=0$ - brak zależności,
- $-1<r_{xy}<0$ - korelacja ujemna niedoskonała (słaba/umiarkowana/silna)
- $r_{xy}=-1$ - korelacja ujemna doskonała.

---

# Dobór i weryfikacja modelu

Pakiet [olsrr](https://cran.r-project.org/web/packages/olsrr/) zawiera narzędzia do analizy modeli liniowych.

- wybór cech do modelu

- badanie współliniowości

- badanie normalności

- analiza wartości odstających

---

# Dobór modelu

Wyróżnia się trzy podejścia do tego zagadnienia:

- ekspercki dobór cech

- budowa wszystkich możliwych modeli i wybór najlepszego według określonego kryterium

- regresja krokowa

---

# Badanie współliniowości

**Współczynnik tolerancji** wskazuje na procent niewyjaśnionej zmienności danej zmiennej przez pozostałe zmienne objaśniające.

**Współczynnik VIF** jest obliczany na podstawie wartości współczynnika tolerancji i wskazuje o ile wariancja szacowanego współczynnika regresji jest podwyższona z powodu współliniowości danej zmiennej objaśniającej z pozostałymi zmiennymi objaśniającymi. Wartość współczynnika VIF powyżej 4 należy uznać za wskazującą na współliniowość.

---

# Wartości odstające

**Miara Cooka** jest obliczana poprzez usunięcie i-tej obserwacji z danych i ponowne obliczenie parametrów regresji. Podsumowuje, jak bardzo wszystkie wartości w modelu regresji zmieniają się po usunięciu i-tej obserwacji. Każda obserwacja, dla której wartość miary Cooka przekracza próg obliczany jako $4/n$ jest traktowana jaka wartość odstająca.

**Reszty studentyzowane** oblicza się, dzieląc resztę przez szacunkowe odchylenie standardowe. Odchylenie standardowe dla każdej reszty jest obliczane z wyłączeniem danej obserwacji. Obserwacje dla których wartość reszty przekracza 3 uznaje się za odstające.

---

class: inverse

# Zadanie

Na podstawie zbioru dotyczącego [50 startupów](http://www.wawrowski.edu.pl/data/startups.xlsx) określ jakie czynniki wpływają na przychód startupów. 

Przyda się pakiet _janitor_ i funkcja `clean_names()` do uporządkowania nazw kolumn.

`r countdown(minutes = 10, seconds = 0, top = 0)`

---

class: inverse, center, middle

# Pytania?